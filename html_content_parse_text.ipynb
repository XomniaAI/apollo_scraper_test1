{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (137.0.7151.122) detected in PATH at /usr/local/bin/chromedriver might not be compatible with the detected chrome version (138.0.7204.169); currently, chromedriver 138.0.7204.168 is recommended for chrome 138.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Navigating to Apollo login page...\n",
      "ðŸ“§ Entering credentials...\n",
      "âœ… Email entered: tom.weijers@xomnia.com\n",
      "âœ… Password entered\n",
      "âœ… Login button clicked\n",
      "â³ Logging in, please wait for Apollo to load after login...\n",
      "ðŸ”„ Navigating to search results page...\n",
      "âœ… Successfully logged in and navigated to search page!\n",
      "ðŸ’¾ Saved complete soup: soup_complete_page_1.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import zip_longest\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "EMAIL = os.getenv('APOLLO_EMAIL')\n",
    "PASSWORD = os.getenv('APOLLO_PASSWORD')\n",
    "URL = os.getenv('APOLLO_URL')\n",
    "\n",
    "class ApolloScraper:\n",
    "    def __init__(self, user_agents, url):\n",
    "        '''Initialize the scraper with user agents, login credentials, and URL.'''\n",
    "        self.user_agents = user_agents\n",
    "        self.url = url\n",
    "        self.driver = self.setup_webdriver()\n",
    "\n",
    "    def setup_webdriver(self):\n",
    "        '''Set up the Chrome WebDriver with random user-agent headers.'''\n",
    "        service = Service()\n",
    "        options = Options()\n",
    "        options.add_argument(f\"user-agent={random.choice(self.user_agents)}\")\n",
    "        # options.add_argument(\"--headless\")\n",
    "        self.driver = webdriver.Chrome(service=service, options=options)\n",
    "        self.driver.maximize_window()\n",
    "        return self.driver\n",
    "\n",
    "    def login(self):\n",
    "        '''Perform login to the Apollo website using email and password.'''\n",
    "        print(\"ðŸ”„ Navigating to Apollo login page...\")\n",
    "        \n",
    "        # Go directly to login page first (like colleague's solution)\n",
    "        self.driver.get(\"https://app.apollo.io/#/login\")\n",
    "        time.sleep(5)  # Wait for page to load\n",
    "        \n",
    "        print(\"ðŸ“§ Entering credentials...\")\n",
    "        \n",
    "        # Use colleague's simple approach - direct CSS selectors, no clear()\n",
    "        try:\n",
    "            # Enter email\n",
    "            email_input = self.driver.find_element(By.CSS_SELECTOR, 'input[name=\"email\"]')\n",
    "            email_input.send_keys(EMAIL)\n",
    "            print(f\"âœ… Email entered: {EMAIL}\")\n",
    "            \n",
    "            # Enter password  \n",
    "            password_input = self.driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\n",
    "            password_input.send_keys(PASSWORD)\n",
    "            print(\"âœ… Password entered\")\n",
    "            \n",
    "            # Click login button\n",
    "            login_button = self.driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]')\n",
    "            login_button.click()\n",
    "            print(\"âœ… Login button clicked\")\n",
    "            \n",
    "            print(\"â³ Logging in, please wait for Apollo to load after login...\")\n",
    "            time.sleep(10)  # Wait for login to complete\n",
    "            \n",
    "            # Now navigate to the actual search page\n",
    "            print(\"ðŸ”„ Navigating to search results page...\")\n",
    "            self.driver.get(URL)  # Use the search URL from .env\n",
    "            time.sleep(7)  # Wait for results to load\n",
    "            \n",
    "            print(\"âœ… Successfully logged in and navigated to search page!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Login failed: {e}\")\n",
    "            print(\"ðŸ” Taking screenshot for debugging...\")\n",
    "            self.driver.save_screenshot(\"debug_login_error.png\")\n",
    "            print(\"ðŸ“¸ Screenshot saved as 'debug_login_error.png'\")\n",
    "            raise\n",
    "\n",
    "    def save_soup_content(self, soup, page_num, save_type=\"all\"):\n",
    "        \"\"\"Save soup content in multiple formats.\"\"\"\n",
    "        \n",
    "        if save_type in [\"all\", \"html\"]:\n",
    "            # Complete soup as HTML\n",
    "            with open(f\"soup_complete_page_{1}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(str(soup))\n",
    "            print(f\"ðŸ’¾ Saved complete soup: soup_complete_page_{page_num}.html\")\n",
    "        \n",
    "        if save_type in [\"all\", \"filtered\"]:\n",
    "            # Save filtered soup (after your cell filtering)\n",
    "            with open(f\"soup_filtered_page_{page_num}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(str(soup))\n",
    "            print(f\"ðŸ’¾ Saved filtered soup: soup_filtered_page_{page_num}.html\")\n",
    "        \n",
    "        if save_type in [\"all\", \"pretty\"]:\n",
    "            # Pretty-printed version for debugging\n",
    "            with open(f\"soup_pretty_page_{page_num}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(soup.prettify())\n",
    "            print(f\"ðŸ’¾ Saved pretty soup: soup_pretty_page_{page_num}.html\")\n",
    "\n",
    "    def get_page_soup(self):\n",
    "        \"\"\"Get BeautifulSoup object from current page source\"\"\"\n",
    "        return BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "    \n",
    "    def quit(self):\n",
    "        \n",
    "        '''Close the WebDriver session.'''\n",
    "        self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_agents = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36\"]\n",
    "\n",
    "    scraper = ApolloScraper(user_agents, URL)\n",
    "\n",
    "    scraper.login()\n",
    "    \n",
    "    # Get soup from current page and save it\n",
    "    soup = scraper.get_page_soup()\n",
    "    scraper.save_soup_content(soup, \"1\", \"html\")\n",
    "    \n",
    "    scraper.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_soup_to_first_three_cells(self, soup):\n",
    "    \"\"\"Filter HTML soup to keep only cells 2, 3, 4 per person row (Name, Job Title, Company).\"\"\"\n",
    "    print(\"ðŸ”§ Filtering HTML to keep only cells 2, 3, 4 per person (Name, Job Title, Company)...\")\n",
    "    \n",
    "    # Find all person rows\n",
    "    person_rows = soup.find_all('div', {'role': 'row'})\n",
    "    print(f\"ðŸ” Found {len(person_rows)} person rows\")\n",
    "    \n",
    "    filtered_rows = []\n",
    "    for row in person_rows:\n",
    "        # Find all cells in this row\n",
    "        cells = row.find_all('div', {'role': 'cell'})\n",
    "        \n",
    "        if len(cells) >= 4:\n",
    "            # Keep cells 2, 3, 4 (skip checkbox cell 1, get Name, Job Title, Company)\n",
    "            relevant_cells = cells[1:4]  # This gets cells at index 1, 2, 3 (which are cells 2, 3, 4 in human counting)\n",
    "            \n",
    "            # Create new row with only relevant cells\n",
    "            new_row = soup.new_tag('div', **row.attrs)\n",
    "            for cell in relevant_cells:\n",
    "                new_row.append(cell)\n",
    "            \n",
    "            filtered_rows.append(new_row)\n",
    "    \n",
    "    # Create new soup with only filtered rows\n",
    "    filtered_soup = BeautifulSoup('<div class=\"filtered-content\"></div>', 'html.parser')\n",
    "    container = filtered_soup.find('div', class_='filtered-content')\n",
    "    \n",
    "    for row in filtered_rows:\n",
    "        container.append(row)\n",
    "    \n",
    "    print(f\"âœ… Filtered to {len(filtered_rows)} rows with cells 2,3,4 each (Name, Job Title, Company)\")\n",
    "    return filtered_soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
